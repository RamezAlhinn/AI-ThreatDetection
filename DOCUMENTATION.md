# AI Threat Detection Agent - Technical Documentation

## ğŸ“š Table of Contents

1. [Project Overview](#project-overview)
2. [Architecture Deep Dive](#architecture-deep-dive)
3. [How LLM Communication Works](#how-llm-communication-works)
4. [File Structure Explained](#file-structure-explained)
5. [Demo Guide](#demo-guide)
6. [Technical Implementation](#technical-implementation)
7. [Extending the System](#extending-the-system)

---

## ğŸ“‹ Project Overview

### What This Project Does

This is an **AI-powered cybersecurity threat detection system** that:
- Analyzes security log entries in real-time
- Uses Large Language Models (LLMs) to classify threats
- Provides human-readable explanations for each decision
- Monitors its own quality with metrics dashboards
- Operates in two modes: Mock (offline) and Real (API-based)

### Key Innovation: Mock Mode

The system can run **completely offline** using pattern-matching rules that simulate LLM behavior. This ensures:
- âœ… Reliable demos without internet
- âœ… No API costs during development
- âœ… Deterministic testing
- âœ… Fast iteration

---

## ğŸ—ï¸ Architecture Deep Dive

### System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  USER INTERFACE                  â”‚
â”‚              (Streamlit Web App)                 â”‚
â”‚  - 5 presentation tabs                           â”‚
â”‚  - Interactive controls                          â”‚
â”‚  - Real-time visualization                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            THREAT DETECTION AGENT                â”‚
â”‚         (threat_agent.py)                        â”‚
â”‚  - Orchestrates the detection pipeline           â”‚
â”‚  - Batches multiple log entries                  â”‚
â”‚  - Adds metadata (timestamps, etc.)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                      â”‚
           â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LLM CLIENT     â”‚    â”‚   STORAGE LAYER     â”‚
â”‚ (llm_client.py)  â”‚    â”‚   (storage.py)      â”‚
â”‚                  â”‚    â”‚                     â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ - Saves predictions â”‚
â”‚ â”‚  MOCK MODE   â”‚ â”‚    â”‚ - Loads labels      â”‚
â”‚ â”‚  Pattern     â”‚ â”‚    â”‚ - CSV format        â”‚
â”‚ â”‚  Matching    â”‚ â”‚    â”‚ - Easy to query     â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚  REAL MODE   â”‚ â”‚
â”‚ â”‚  HTTP calls  â”‚ â”‚
â”‚ â”‚  to HF API   â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HUGGING FACE API    â”‚
â”‚  (External Service)  â”‚
â”‚  - Mistral-7B model  â”‚
â”‚  - Free tier         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

**Step-by-step process:**

1. **Input**: User provides log entry
   ```
   "2024-01-15 10:30:22 ERROR user=unknown action=sql_query query=\"SELECT * FROM users WHERE '1'='1'\""
   ```

2. **Routing**: UI calls `ThreatDetectionAgent.analyze_log()`

3. **LLM Selection**: Agent checks config â†’ uses Mock or Real mode

4. **Classification**:
   - **Mock Mode**: Pattern matching against known attack signatures
   - **Real Mode**: HTTP POST to Hugging Face API with structured prompt

5. **Response Parsing**: Extract prediction, confidence, explanation, action

6. **Metadata Addition**: Add timestamp, original log

7. **Storage**: Save to CSV for quality monitoring

8. **Display**: Show results in UI with color coding

---

## ğŸ¤– How LLM Communication Works

### Mock Mode (Default)

**Location**: `src/llm_client.py` â†’ `_mock_classify()`

**How it works:**

1. **Pattern Detection**: Uses regex to match attack patterns
   ```python
   malicious_patterns = [
       (r"sql.*injection", "SQL injection attempt detected"),
       (r"privilege.?escalation", "Unauthorized privilege escalation"),
       # ... more patterns
   ]
   ```

2. **Priority Matching**: Checks malicious â†’ suspicious â†’ benign
   - First match wins
   - Returns pre-defined response

3. **Response Structure**:
   ```python
   {
       "prediction": "malicious",
       "confidence": 0.92,
       "explanation": "SQL injection pattern detected...",
       "recommended_action": "URGENT: Block source IP..."
   }
   ```

**Why Mock Mode?**
- No network required
- Instant responses (< 1ms)
- Perfect for demos, testing, development
- Deterministic behavior

---

### Real Mode (Optional)

**Location**: `src/llm_client.py` â†’ `_real_classify()`

**How it works:**

#### 1. Prompt Construction

We build a structured prompt for the LLM:

```python
prompt = f"""You are a cybersecurity expert analyzing system logs for threats.

Classify the following log entry as either "benign", "suspicious", or "malicious".
Provide a brief explanation and recommended action.

Log Entry:
{log_entry}

Respond in this exact format:
PREDICTION: [benign/suspicious/malicious]
CONFIDENCE: [0.0-1.0]
EXPLANATION: [Brief explanation of your reasoning]
RECOMMENDED_ACTION: [What should be done about this event]

Your response:"""
```

**Key Design Decisions:**
- Clear instructions reduce ambiguity
- Structured output format enables parsing
- Domain context ("cybersecurity expert") improves accuracy

#### 2. API Request

We make an HTTP POST to Hugging Face Inference API:

```python
headers = {
    "Authorization": f"Bearer {settings.hf_api_key}",
    "Content-Type": "application/json"
}

payload = {
    "inputs": prompt,
    "parameters": {
        "max_new_tokens": 200,      # Limit response length
        "temperature": 0.3,          # Low = more deterministic
        "return_full_text": False    # Only new generation
    }
}

response = requests.post(
    "https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2",
    headers=headers,
    json=payload,
    timeout=30
)
```

**Parameters Explained:**
- `max_new_tokens`: Prevents overly long responses
- `temperature`: Controls randomness (0 = deterministic, 1 = creative)
- `timeout`: Prevents hanging requests

#### 3. Response Parsing

Extract structured fields from LLM text:

```python
prediction_match = re.search(
    r"PREDICTION:\s*(benign|suspicious|malicious)",
    llm_output,
    re.IGNORECASE
)
```

#### 4. Error Handling

Graceful fallback to mock mode if:
- Network timeout (30s)
- API error (503, 429, etc.)
- Parsing fails
- Invalid API key

```python
try:
    response = requests.post(...)
except requests.exceptions.Timeout:
    print("âš ï¸  API timeout. Using mock response.")
    return self._mock_classify(log_entry)
```

---

### Why Two Modes?

| Aspect | Mock Mode | Real Mode |
|--------|-----------|-----------|
| **Speed** | < 1ms | 200-1000ms |
| **Cost** | Free | Free tier limited |
| **Network** | Not required | Required |
| **Accuracy** | Rule-based | LLM intelligence |
| **Use Case** | Demos, testing | Production, exploration |

**Best Practice**: Develop with Mock, deploy with Real (or hybrid)

---

## ğŸ“ File Structure Explained

### `/src/` - Core Application

#### `config.py` - Configuration Management
```python
# What it does:
- Loads environment variables (.env file)
- Provides settings object used by all modules
- Validates configuration (API keys, paths, etc.)

# Key settings:
- HF_API_KEY: Hugging Face API token
- HF_API_URL: Model endpoint
- USE_MOCK_MODEL: Toggle between modes
- STORAGE_PATH: Where to save predictions
```

#### `llm_client.py` - LLM Abstraction Layer
```python
# What it does:
- Abstracts LLM communication behind clean interface
- Implements both Mock and Real modes
- Handles errors gracefully

# Key functions:
- classify_log(log_entry) â†’ classification dict
- _mock_classify() â†’ pattern-based
- _real_classify() â†’ API-based
- _build_prompt() â†’ prompt engineering
```

#### `threat_agent.py` - Business Logic
```python
# What it does:
- Main orchestrator for threat detection
- Batch processing support
- Summary statistics

# Key functions:
- analyze_log() â†’ single log analysis
- analyze_logs_batch() â†’ multiple logs
- get_summary_stats() â†’ aggregated metrics
```

#### `storage.py` - Data Persistence
```python
# What it does:
- Save predictions to CSV
- Load labeled data for evaluation
- Manage prediction history

# Key functions:
- save_prediction() â†’ append to CSV
- load_all_predictions() â†’ read history
- load_labeled_data() â†’ ground truth dataset
```

#### `metrics.py` - Quality Evaluation
```python
# What it does:
- Compute accuracy, precision, recall
- Generate confusion matrix
- Extract interesting mistakes (false positives/negatives)

# Key functions:
- compute_accuracy()
- compute_confusion_matrix()
- extract_mistakes()
- generate_summary_report()
```

#### `ui_app.py` - Streamlit Interface
```python
# What it does:
- 5-tab presentation interface
- Interactive controls
- Real-time visualization

# Tabs:
1. Overview - Architecture & purpose
2. Live Demo - Interactive threat detection
3. Quality - Metrics dashboard
4. Frameworks - Comparison methodology
5. Future - Extensions & job alignment
```

---

### `/data/` - Sample Data

#### `sample_logs.txt`
- 15 synthetic security log entries
- Mix of benign, suspicious, malicious
- Used in Live Demo tab

#### `labeled_logs.csv`
- 20 labeled examples (ground truth)
- Columns: id, log, true_label
- Used for quality evaluation

---

### `/tests/` - Unit Tests

#### `test_metrics.py`
- Tests accuracy calculation
- Tests confusion matrix
- Tests mistake extraction

#### `test_threat_agent.py`
- Tests agent in mock mode
- Tests batch processing
- Tests summary statistics

**Run tests**: `make test` or `pytest tests/`

---

## ğŸ¯ Demo Guide

### Preparation (2 minutes before demo)

1. **Start the app**:
   ```bash
   cd AI-ThreatDetection
   make run
   # or: streamlit run src/ui_app.py
   ```

2. **Verify it opens** at `http://localhost:8501`

3. **Check status** in sidebar: Should show "ğŸ­ Mock Mode Active"

---

### Demo Script (15 minutes)

#### Tab 1: Overview (3 min)

**What to say:**
> "This is an AI-powered threat detection system I built for the Siemens interview. It demonstrates three core skills: evaluating AI frameworks, building PoC agents, and monitoring quality."

**What to show:**
- Point to the 3 capability cards
- Briefly explain the architecture flow (Input â†’ Processing â†’ Output)
- Mention mock mode ensures reliability

#### Tab 2: Live Demo (5 min)

**What to say:**
> "Let me show you how it works in real-time. I'll analyze some security logs."

**What to do:**
1. Keep default selection (logs 1, 5, 8) or pick:
   - One benign (e.g., "alice login success")
   - One malicious (e.g., SQL injection or privilege escalation)

2. Click "ğŸ” Analyze Logs"

3. **Point out**:
   - Summary metrics at top
   - Color-coded results (green/yellow/red)
   - Natural language explanations
   - Actionable recommendations

**What to say:**
> "Notice it doesn't just say 'malicious'â€”it explains WHY and suggests what to do next. This is critical for analyst trust."

#### Tab 3: Quality Monitoring (4 min)

**What to say:**
> "AI systems need continuous monitoring. This tab shows how we evaluate performance."

**What to do:**
1. Click "ğŸ”„ Run Evaluation"
2. Wait 5-10 seconds for results

**What to show:**
- Overall accuracy metric
- Per-class precision/recall table
- Confusion matrix
- Critical mistakes section

**What to say:**
> "We have 90%+ accuracy, but we pay special attention to false negativesâ€”missed threats are critical in security. This dashboard helps us identify where to improve."

#### Tab 4: Frameworks (2 min)

**What to say:**
> "I evaluated multiple approaches: free LLM APIs, self-hosted models, rule-based systems, and classical ML."

**What to show:**
- Comparison table

**What to say:**
> "I chose a free LLM API with mock mode fallback. This gives us natural language explanations, zero infrastructure, and demo reliability. In production, we could easily swap to a self-hosted model for privacy."

#### Tab 5: Future Work (1 min)

**What to say:**
> "This PoC is intentionally minimal, but it's designed to scale. Here are four extension directions..."

**What to show:**
- Quickly mention multi-agent system, human-in-the-loop, monitoring, enterprise integration

**What to say:**
> "I'm excited to bring this approach to Siemensâ€”starting small, iterating fast, and always prioritizing quality."

---

### Handling Questions

**Q: "Is this using a real LLM?"**
> "It's in mock mode for demo reliability, but I've tested it with the Hugging Face API using Mistral-7B. You can switch modes with one environment variable."

**Q: "How accurate is it?"**
> "On our test set, 90%+ accuracy. The mock mode uses pattern matching tuned to common attack signatures. A real LLM would handle novel attacks better."

**Q: "Could this work with sensitive data?"**
> "Great question. The free API sends data externally, which is fine for this PoC with synthetic logs. For production, we'd use a self-hosted LLM or on-premises deployment. The architecture supports swapping backends easily."

**Q: "How long did this take to build?"**
> "About [X days] for the full PoC including UI, tests, and documentation. The modular design made it fastâ€”each component is independent and testable."

---

## ğŸ”§ Technical Implementation

### How Mock Mode Works Internally

**Location**: `src/llm_client.py` lines 60-125

**Algorithm**:

1. **Normalize input**: Convert log to lowercase
   ```python
   log_lower = log_entry.lower()
   ```

2. **Check malicious patterns** (priority 1):
   ```python
   for pattern, reason in malicious_patterns:
       if re.search(pattern, log_lower):
           return malicious_response
   ```

3. **Check suspicious patterns** (priority 2):
   ```python
   for pattern, reason in suspicious_patterns:
       if re.search(pattern, log_lower):
           return suspicious_response
   ```

4. **Default to benign** (priority 3):
   ```python
   return benign_response
   ```

**Pattern Examples**:
```python
malicious_patterns = [
    (r"sql.*injection", "SQL injection attempt"),
    (r"privilege.?escalation", "Unauthorized privilege escalation"),
    (r"port.?scan", "Network port scanning"),
    (r"buffer.?overflow", "Buffer overflow attack"),
]

suspicious_patterns = [
    (r"failed.*attempts?=[3-9]", "Multiple failed login attempts"),
    (r"config.?change", "Configuration modification"),
]
```

---

### How Real Mode API Calls Work

**Request Format**:
```json
POST https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2

Headers:
{
  "Authorization": "Bearer hf_xxxxxxxxxxxxx",
  "Content-Type": "application/json"
}

Body:
{
  "inputs": "<FULL_PROMPT>",
  "parameters": {
    "max_new_tokens": 200,
    "temperature": 0.3,
    "return_full_text": false
  }
}
```

**Response Format**:
```json
[
  {
    "generated_text": "PREDICTION: malicious\nCONFIDENCE: 0.95\nEXPLANATION: SQL injection pattern detected...\nRECOMMENDED_ACTION: Block IP immediately..."
  }
]
```

**Parsing Logic**:
```python
# Extract prediction
prediction = re.search(r"PREDICTION:\s*(benign|suspicious|malicious)", text).group(1)

# Extract confidence
confidence = float(re.search(r"CONFIDENCE:\s*(0?\.\d+)", text).group(1))

# Extract explanation
explanation = re.search(r"EXPLANATION:\s*(.+?)(?=\nRECOMMENDED)", text).group(1)
```

---

### Storage Format

**predictions.csv**:
```csv
id,timestamp,log,prediction,confidence,explanation,recommended_action,human_label
1,2024-01-15T10:30:22,ERROR sql_query...,malicious,0.92,SQL injection detected...,URGENT: Block IP,...,
2,2024-01-15T10:31:10,INFO user=alice login,benign,0.95,Normal user activity...,NONE: Continue monitoring,...,
```

**Why CSV?**
- Simple, portable, human-readable
- Easy to import into Excel/Google Sheets
- Pandas integration
- Can switch to SQLite with minimal code changes

---

## ğŸš€ Extending the System

### Adding a New LLM Backend

**Example: Add OpenAI GPT-4**

1. **Update `config.py`**:
   ```python
   openai_api_key: Optional[str] = Field(default=None)
   llm_backend: str = Field(default="huggingface")  # or "openai"
   ```

2. **Add method in `llm_client.py`**:
   ```python
   def _openai_classify(self, log_entry: str):
       import openai
       openai.api_key = settings.openai_api_key

       response = openai.ChatCompletion.create(
           model="gpt-4",
           messages=[{"role": "user", "content": self._build_prompt(log_entry)}]
       )

       return self._parse_llm_response(response.choices[0].message.content)
   ```

3. **Update routing**:
   ```python
   def classify_log(self, log_entry: str):
       if settings.llm_backend == "openai":
           return self._openai_classify(log_entry)
       elif self.use_mock:
           return self._mock_classify(log_entry)
       else:
           return self._real_classify(log_entry)
   ```

---

### Adding Human Feedback

1. **Update storage schema**:
   ```python
   headers = ["id", ..., "human_label", "human_feedback", "reviewed_by"]
   ```

2. **Add UI controls**:
   ```python
   if st.button("ğŸ‘ Correct"):
       store.update_prediction(id, human_label=prediction, feedback="correct")
   if st.button("ğŸ‘ Wrong"):
       correct_label = st.selectbox("Correct label:", ["benign", "suspicious", "malicious"])
       store.update_prediction(id, human_label=correct_label, feedback="incorrect")
   ```

3. **Use feedback for improvement**:
   - Identify patterns where model fails
   - Add new rules to mock mode
   - Fine-tune prompts for real mode
   - Train a custom model

---

### Switching to SQLite

**Replace `storage.py` implementation**:

```python
import sqlite3

class PredictionStore:
    def __init__(self, db_path="data/predictions.db"):
        self.conn = sqlite3.connect(db_path)
        self._create_table()

    def _create_table(self):
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS predictions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT,
                log TEXT,
                prediction TEXT,
                confidence REAL,
                explanation TEXT,
                recommended_action TEXT,
                human_label TEXT
            )
        """)

    def save_prediction(self, result):
        self.conn.execute("""
            INSERT INTO predictions (timestamp, log, prediction, confidence, explanation, recommended_action)
            VALUES (?, ?, ?, ?, ?, ?)
        """, (result['timestamp'], result['log'], result['prediction'], ...))
        self.conn.commit()
```

**Why SQLite?**
- Better for large datasets (100k+ predictions)
- Supports complex queries
- Concurrent access
- Still lightweight (single file)

---

## ğŸ“Š Performance Considerations

### Mock Mode Performance
- **Latency**: < 1ms per log
- **Throughput**: ~100k logs/second (single thread)
- **Memory**: Negligible (stateless)

### Real Mode Performance
- **Latency**: 200-1000ms per log (network + inference)
- **Throughput**: ~1-5 logs/second (free tier)
- **Rate Limits**:
  - Hugging Face free tier: ~100 requests/hour
  - Can upgrade for higher limits

### Optimization Strategies

1. **Batch Processing**: Send multiple logs in one request
   ```python
   # Instead of N requests for N logs, send 1 request
   prompt = f"Analyze these {len(logs)} logs:\n" + "\n".join(logs)
   ```

2. **Caching**: Store results for identical logs
   ```python
   cache = {}
   if log in cache:
       return cache[log]
   result = llm_classify(log)
   cache[log] = result
   ```

3. **Hybrid Mode**: Use mock for known patterns, LLM for unknowns
   ```python
   if matches_known_pattern(log):
       return mock_classify(log)  # Fast
   else:
       return llm_classify(log)   # Slow but smart
   ```

---

## ğŸ“ Key Learnings

### What Worked Well
1. **Mock mode**: Made development/demos reliable
2. **Modular design**: Easy to test and extend
3. **Structured prompts**: Improved LLM output quality
4. **CSV storage**: Simple, portable, good enough

### What Could Be Improved
1. **Error messages**: More user-friendly
2. **Prompt engineering**: Could fine-tune for better accuracy
3. **Visualization**: Add charts, graphs
4. **Real-time mode**: Stream logs as they arrive

### Interview Talking Points
- "I prioritized demo reliability over using the fanciest tech"
- "The abstraction layer means we can easily compare different LLMs"
- "Quality monitoring is built in from day one, not added later"
- "This architecture scales from PoC to production"

---

## ğŸ”— Resources

### Hugging Face API
- Docs: https://huggingface.co/docs/api-inference
- Models: https://huggingface.co/models
- Free tier: https://huggingface.co/pricing

### Streamlit
- Docs: https://docs.streamlit.io
- Gallery: https://streamlit.io/gallery

### Security Concepts
- MITRE ATT&CK: https://attack.mitre.org
- OWASP Top 10: https://owasp.org/Top10

---

## ğŸ“ Quick Reference

### Run the App
```bash
make run
# or
streamlit run src/ui_app.py
```

### Run Tests
```bash
make test
# or
pytest tests/ -v
```

### Switch Modes
```bash
# Edit .env file
USE_MOCK_MODEL=false  # Real mode
USE_MOCK_MODEL=true   # Mock mode (default)
```

### Add New Patterns
Edit `src/llm_client.py`, add to `malicious_patterns` or `suspicious_patterns`:
```python
(r"your_regex_pattern", "Description of threat")
```

---

**Built with â¤ï¸ for the Siemens AI/Cybersecurity Working Student Interview**
